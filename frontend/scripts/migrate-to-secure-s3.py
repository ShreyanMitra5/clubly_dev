#!/usr/bin/env python3
"""
Migration Script: Old Bucket to Secure S3 Setup

This script helps migrate existing files from the old bucket (clubly-slides) 
to the new secure bucket (clubly-prod) with proper tenant isolation.
"""

import boto3
import os
import json
from datetime import datetime
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

def get_s3_client(region='us-west-1'):
    """Get S3 client with credentials"""
    return boto3.client(
        's3',
        region_name=region,
        aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),
        aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY')
    )

def analyze_old_bucket():
    """Analyze the old bucket structure to understand what needs migration"""
    
    old_bucket = 'clubly-slides'
    new_bucket = os.getenv('S3_BUCKET', 'clubly-prod')
    
    print(f"üîç Analyzing old bucket: {old_bucket}")
    
    try:
        s3 = get_s3_client()
        
        # List all objects in the old bucket
        paginator = s3.get_paginator('list_objects_v2')
        page_iterator = paginator.paginate(Bucket=old_bucket)
        
        files_by_type = {
            'presentations': [],
            'thumbnails': [],
            'meeting-notes': [],
            'task-history': [],
            'email-history': [],
            'other': []
        }
        
        for page in page_iterator:
            if 'Contents' in page:
                for obj in page['Contents']:
                    key = obj['Key']
                    size = obj['Size']
                    
                    # Categorize files
                    if 'history' in key:
                        if 'task' in key:
                            files_by_type['task-history'].append({'key': key, 'size': size})
                        elif 'email' in key:
                            files_by_type['email-history'].append({'key': key, 'size': size})
                        elif 'meeting' in key:
                            files_by_type['meeting-notes'].append({'key': key, 'size': size})
                    elif key.endswith('.pptx') or key.endswith('.ppt'):
                        files_by_type['presentations'].append({'key': key, 'size': size})
                    elif key.endswith('.png') or key.endswith('.jpg'):
                        files_by_type['thumbnails'].append({'key': key, 'size': size})
                    else:
                        files_by_type['other'].append({'key': key, 'size': size})
        
        # Print analysis
        print(f"\nüìä File Analysis:")
        total_files = 0
        total_size = 0
        
        for file_type, files in files_by_type.items():
            count = len(files)
            size = sum(f['size'] for f in files)
            total_files += count
            total_size += size
            
            print(f"   {file_type}: {count} files, {size / (1024*1024):.2f} MB")
        
        print(f"\nüìà Total: {total_files} files, {total_size / (1024*1024):.2f} MB")
        
        return files_by_type
        
    except Exception as e:
        print(f"‚ùå Error analyzing bucket: {e}")
        return None

def generate_migration_plan(files_by_type):
    """Generate a migration plan for the files"""
    
    print(f"\nüìã Migration Plan:")
    
    plan = {
        'phase1': {
            'description': 'User data files (task history, email history, meeting notes)',
            'files': files_by_type['task-history'] + files_by_type['email-history'] + files_by_type['meeting-notes'],
            'priority': 'High',
            'notes': 'These contain user data and need careful migration with proper tenant isolation'
        },
        'phase2': {
            'description': 'Presentation files',
            'files': files_by_type['presentations'],
            'priority': 'Medium',
            'notes': 'Need to determine club ownership from metadata or database'
        },
        'phase3': {
            'description': 'Thumbnail files',
            'files': files_by_type['thumbnails'],
            'priority': 'Low',
            'notes': 'Can be regenerated if needed'
        }
    }
    
    for phase, details in plan.items():
        print(f"\n   {phase.upper()}:")
        print(f"     Description: {details['description']}")
        print(f"     Priority: {details['priority']}")
        print(f"     Files: {len(details['files'])}")
        print(f"     Notes: {details['notes']}")
    
    return plan

def create_migration_script(plan):
    """Create a migration script based on the plan"""
    
    script_content = '''#!/usr/bin/env python3
"""
Auto-generated Migration Script for Clubly S3 Migration

This script was generated by the migration analyzer.
Please review and customize before running.
"""

import boto3
import os
import json
from datetime import datetime
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

def migrate_files():
    """Migrate files from old bucket to new secure bucket"""
    
    old_bucket = 'clubly-slides'
    new_bucket = os.getenv('S3_BUCKET', 'clubly-prod')
    
    s3 = boto3.client(
        's3',
        region_name=os.getenv('AWS_REGION', 'us-west-1'),
        aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),
        aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY')
    )
    
    # Migration mapping - customize this based on your needs
    migration_mapping = {
        # Example: old_key -> new_key
        # 'user_123/task_history.json': 'schools/school1/clubs/club1/users/user_123/2024/01/15/task_history.json'
    }
    
    print(f"üöÄ Starting migration from {old_bucket} to {new_bucket}")
    
    for old_key, new_key in migration_mapping.items():
        try:
            print(f"üìÅ Migrating: {old_key} -> {new_key}")
            
            # Copy object to new bucket
            copy_source = {'Bucket': old_bucket, 'Key': old_key}
            s3.copy_object(
                CopySource=copy_source,
                Bucket=new_bucket,
                Key=new_key,
                Metadata={
                    'migrated-from': old_key,
                    'migration-date': datetime.now().isoformat(),
                    'original-bucket': old_bucket
                }
            )
            
            print(f"‚úÖ Successfully migrated: {old_key}")
            
        except Exception as e:
            print(f"‚ùå Failed to migrate {old_key}: {e}")
    
    print("üéâ Migration completed!")

if __name__ == "__main__":
    print("‚ö†Ô∏è  WARNING: This is an auto-generated migration script!")
    print("Please review and customize before running.")
    print("Make sure you have proper backups and understand the implications.")
    
    response = input("\\nDo you want to proceed with migration? (yes/no): ")
    if response.lower() == 'yes':
        migrate_files()
    else:
        print("Migration cancelled.")
'''
    
    # Write the script
    script_path = 'frontend/scripts/execute-migration.py'
    with open(script_path, 'w') as f:
        f.write(script_content)
    
    print(f"\nüìù Migration script created: {script_path}")
    print("   Please review and customize before running!")

def main():
    """Main migration analysis function"""
    
    print("üöÄ Clubly S3 Migration Analysis Tool")
    print("=" * 50)
    
    # Check environment
    if not os.getenv('AWS_ACCESS_KEY_ID') or not os.getenv('AWS_SECRET_ACCESS_KEY'):
        print("‚ùå AWS credentials not found in environment variables")
        print("Please set AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY")
        return
    
    # Analyze old bucket
    files_by_type = analyze_old_bucket()
    if not files_by_type:
        return
    
    # Generate migration plan
    plan = generate_migration_plan(files_by_type)
    
    # Create migration script
    create_migration_script(plan)
    
    print(f"\nüí° Next Steps:")
    print("   1. Review the generated migration script")
    print("   2. Customize the migration mapping based on your data structure")
    print("   3. Test with a small subset of files first")
    print("   4. Run the migration during low-traffic hours")
    print("   5. Verify all files are accessible in the new bucket")
    print("   6. Update your application to use the new secure endpoints")

if __name__ == "__main__":
    main()
